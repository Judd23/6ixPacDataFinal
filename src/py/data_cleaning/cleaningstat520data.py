# -*- coding: utf-8 -*-
"""CleaningStat520Data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l8_gNw9Zm1RPdp-yglwzVR6IFqheEr_s
"""

# mount google drive
from google.colab import drive
drive.mount('/content/drive')

# import libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.miscmodels.ordinal_model import OrderedModel

!pip install semopy

# import Condensed Data from directory
df = pd.read_csv('/content/drive/MyDrive/STAT520/NewDatasetRCs.csv')

selected_columns = ["SLFCHG01",
             "SLFCHG02",
             "SLFCHG03",
             "SLFCHG04",
              "SLFCHG09",
             "CSSRAT01",
             "CSSRAT07",
             "SUCCESS4",
              "SUCCESS7",
             "SATIS13",
             "SATIS01",
             "SATIS02",
             "SATIS07",
             "SATIS15",
             "SATIS25",
              "SATIS28",
             "CSSRAT16",
             "GENACT05",
             "COLACT19",
             "COLACT17",
             "FATHEDUC",
             "MOTHEDUC",
             "RACEGROUP",
             "UndrRepStud_RC",
             "FINCON_RC",
             "FirstGen_RC",
             "INCOME",
             "HSGPA",
             "CITIZEN",
             "PLANLIVE",
             "DSstud_RC",
             "SIFRAT01",
             "SIFRAT07",
             "SIFMAJA",
             "CSSHPW01",
             "CSSHPW02",
             "CSSHPW03",
             "CSSHPW05",
             "CSSHPW08",
              "COLLGPA"]

CondensedData = df[selected_columns]
display(CondensedData.head())

# Save the new DataFrame to a new CSV file
CondensedData.to_csv('/content/drive/MyDrive/STAT520/CondensedData.csv', index=False)

original_df = pd.read_csv('/content/drive/MyDrive/STAT520/CondensedData.csv', low_memory=False)
# List of columns that previously had missing values after initial filtering
cols_with_potential_missing = [
    "SLFCHG01",
             "SLFCHG02",
             "SLFCHG03",
             "SLFCHG04",
              "SLFCHG09",
             "CSSRAT01",
             "CSSRAT07",
             "SUCCESS4",
              "SUCCESS7",
             "SATIS13",
             "SATIS01",
             "SATIS02",
             "SATIS07",
             "SATIS15",
             "SATIS25",
              "SATIS28",
             "CSSRAT16",
             "GENACT05",
             "COLACT19",
             "COLACT17",
             "FATHEDUC",
             "MOTHEDUC",
             "RACEGROUP",
             "UndrRepStud_RC",
             "FINCON_RC",
             "FirstGen_RC",
             "INCOME",
             "HSGPA",
             "CITIZEN",
             "PLANLIVE",
             "DSstud_RC",
             "SIFRAT01",
             "SIFRAT07",
             "SIFMAJA",
             "CSSHPW01",
             "CSSHPW02",
             "CSSHPW03",
             "CSSHPW05",
             "CSSHPW08",
             "COLLGPA"
]
# Check for common missing value representations in these columns
missing_indicators = ["", " ", "NA", "NULL", "#N/A", "N/A", "?"]

print("Checking for other potential missing value representations in the original dataset:")

missing_data_counts = {}

for col in cols_with_potential_missing:
    if col in original_df.columns:
        missing_data_counts[col] = {}
        for indicator in missing_indicators:
            count = (original_df[col] == indicator).sum()
            if count > 0:
                missing_data_counts[col][f"'{indicator}'"] = count
    else:
        print(f"\nColumn '{col}' not found in the original dataset.")

# Convert the dictionary to a DataFrame
missing_data_table = pd.DataFrame.from_dict(missing_data_counts, orient='index')

# Fill NaN values with 0 for indicators that weren't found in a column
missing_data_table = missing_data_table.fillna(0).astype(int)

# Calculate the total number of missing values for each column
missing_data_table['Total Missing'] = missing_data_table.sum(axis=1)

# Calculate the percentage of missing values and format as a percentage string
total_rows = len(original_df)
missing_data_table['Percentage Missing'] = (missing_data_table['Total Missing'] / total_rows) * 100
missing_data_table['Percentage Missing'] = missing_data_table['Percentage Missing'].map('{:.2f}%'.format)

# Display the table
print("\nTable of missing value counts by indicator:")
display(missing_data_table)

# Define the missing value indicators identified earlier
missing_indicators = ["", " ", "NA", "NULL", "#N/A", "N/A", "?"]

# Replace the specified missing value indicators with NaN in the DataFrame
CondensedData_cleaned = CondensedData.replace(missing_indicators, np.nan)

# Calculate the number of missing values per row in the cleaned DataFrame
missing_variables_per_row_cleaned = CondensedData_cleaned.isnull().sum(axis=1)

# Display the number of missing variables for the first few rows of the cleaned data
print("Number of missing variables per row (first 10 rows) after converting missing values:")
display(missing_variables_per_row_cleaned.head(10))

# Optionally, display a summary of the missing values per row in the cleaned data
print("\nSummary of missing variables per row after converting missing values:")
display(missing_variables_per_row_cleaned.describe())

# Display the total number of missing values in the cleaned DataFrame to confirm
total_missing_values_cleaned = CondensedData_cleaned.isnull().sum().sum()
print(f"\nTotal number of missing values in the cleaned dataset: {total_missing_values_cleaned}")

# Filter out rows with 15 or more missing values from the cleaned DataFrame
CondensedData_filtered = CondensedData_cleaned[missing_variables_per_row_cleaned < 15].copy()

# Display the head of the filtered DataFrame
print("Filtered DataFrame (first 5 rows):")
display(CondensedData_filtered.head())

# Display the shape of the original cleaned and filtered DataFrames to show how many rows were removed
print(f"\nOriginal cleaned DataFrame shape: {CondensedData_cleaned.shape}")
print(f"Filtered DataFrame shape: {CondensedData_filtered.shape}")

# Calculate and display the number of deleted observations
deleted_observations_count = CondensedData_cleaned.shape[0] - CondensedData_filtered.shape[0]
print(f"Number of deleted observations: {deleted_observations_count}")

# Calculate the number of missing values (NaN) for each column in the filtered DataFrame
missing_values_per_column_filtered = CondensedData_filtered.isnull().sum()

# Convert the Series to a DataFrame for easier display
missing_values_table_filtered = missing_values_per_column_filtered.to_frame(name='NaN Count')

# Calculate the percentage of missing values for each column
total_rows_filtered = len(CondensedData_filtered)
missing_values_table_filtered['Percentage NaN'] = (missing_values_table_filtered['NaN Count'] / total_rows_filtered) * 100

# Format the 'Percentage NaN' column as a percentage string
missing_values_table_filtered['Percentage NaN'] = missing_values_table_filtered['Percentage NaN'].map('{:.2f}%'.format)


# Display the table
print("Table of NaN counts and percentages per column in the filtered dataset:")
display(missing_values_table_filtered)

# Save the filtered DataFrame to a new CSV file
CondensedData_filtered.to_csv('/content/drive/MyDrive/STAT520/CondensedData_Filtered.csv', index=False)

print("Filtered data saved to 'CondensedData_Filtered.csv'")

import pandas as pd

# Load the filtered dataset
CondensedData = pd.read_csv("/content/drive/MyDrive/STAT520/CondensedData_Filtered.csv", low_memory=False)

CondensedData_imputed = CondensedData.copy()

# Define variable groups
ordinal_vars = [
    "SLFCHG01","SLFCHG02","SLFCHG03","SLFCHG04","SLFCHG09",
    "CSSRAT01","CSSRAT07","SUCCESS4","SUCCESS7",
    "SATIS13","SATIS01","SATIS02","SATIS07",
    "SATIS15","SATIS25","SATIS28","CSSRAT16","GENACT05",
    "HSGPA","COLLGPA","CITIZEN","PLANLIVE",
    "SIFRAT01","SIFRAT07",
    "CSSHPW01","CSSHPW02","CSSHPW03","CSSHPW05","CSSHPW08"
]

nominal_vars = [
    "FATHEDUC","MOTHEDUC","RACEGROUP",
    "FINCON_RC","FirstGen_RC","INCOME","SIFMAJA"
]

excluded_vars = ["COLACT19","COLACT17","UndrRepStud_RC","DSstud_RC"]

# Create a list to store imputation summary
summary = []

# Median imputation for ordinal variables with <2% missing
for col in ordinal_vars:
    if col in CondensedData_imputed.columns:
        CondensedData_imputed[col] = pd.to_numeric(CondensedData_imputed[col], errors="coerce")
        miss_pct = CondensedData_imputed[col].isna().mean() * 100
        if miss_pct < 2:
            median_val = CondensedData_imputed[col].median()
            CondensedData_imputed[col] = CondensedData_imputed[col].fillna(median_val)
            summary.append((col, round(miss_pct, 2), "Median", median_val))
        else:
            summary.append((col, round(miss_pct, 2), "Skipped", None))

# Mode imputation for nominal variables with <2% missing
for col in nominal_vars:
    if col in CondensedData_imputed.columns:
        miss_pct = CondensedData_imputed[col].isna().mean() * 100
        if miss_pct < 2:
            mode_val = CondensedData_imputed[col].mode(dropna=True)
            if not mode_val.empty:
                CondensedData_imputed[col] = CondensedData_imputed[col].fillna(mode_val[0])
                summary.append((col, round(miss_pct, 2), "Mode", mode_val[0]))
        else:
            summary.append((col, round(miss_pct, 2), "Skipped", None))

# Display summary table
summary_df = pd.DataFrame(summary, columns=["Variable", "Missing %", "Action", "Value Used"])
print(summary_df)

# Calculate the number of missing values (NaN) for each column in the imputed DataFrame
missing_values_per_column_imputed = CondensedData_imputed.isnull().sum()

# Convert the Series to a DataFrame for easier display
missing_values_table_imputed = missing_values_per_column_imputed.to_frame(name='NaN Count')

# Calculate the percentage of missing values for each column
total_rows_imputed = len(CondensedData_imputed)
missing_values_table_imputed['Percentage NaN'] = (missing_values_table_imputed['NaN Count'] / total_rows_imputed) * 100

# Format the 'Percentage NaN' column as a percentage string
missing_values_table_imputed['Percentage NaN'] = missing_values_table_imputed['Percentage NaN'].map('{:.2f}%'.format)


# Display the table
print("Table of NaN counts and percentages per column in the imputed dataset:")
display(missing_values_table_imputed)

# Save the imputed DataFrame to a new CSV file
CondensedData_imputed.to_csv('/content/drive/MyDrive/STAT520/CondensedData_Imputed.csv', index=False)

print("Imputed data saved to 'CondensedData_Imputed.csv'")

# Filter the missing values table to include only variables with missing values
missing_values_table_remaining = missing_values_table_imputed[missing_values_table_imputed['NaN Count'] > 0].copy()

# Display the table
print("Table of NaN counts and percentages for variables with remaining missing values:")
display(missing_values_table_remaining)

import pandas as pd
import numpy as np
from scipy import stats

def little_mcar_test(data):
    """Performs a simplified version of Little's MCAR test for numeric/coded data."""
    data = data.copy()
    patterns = data.isnull().astype(int).drop_duplicates()
    chi_square = 0
    df = 0
    mu_hat = data.mean(skipna=True)
    sigma_hat = np.cov(data.dropna().T, bias=True)

    for _, pattern in patterns.iterrows():
        miss = pattern == 1
        group = data.loc[(data.isnull() == miss).all(axis=1)]
        if len(group) == 0 or miss.all():
            continue
        obs = data.columns[~miss]
        n_g = len(group)
        mu_g = group[obs].mean(skipna=True)
        diff = (mu_g - mu_hat[obs]).to_numpy()[None, :]
        sigma_inv = np.linalg.pinv(sigma_hat[np.ix_(~miss, ~miss)])
        chi_square += n_g * diff @ sigma_inv @ diff.T
        df += len(obs)
    p_value = 1 - stats.chi2.cdf(chi_square, df)
    return {"chi_square": float(chi_square), "df": int(df), "p_value": float(p_value)}

# ---- Run on your variables ----
df = pd.read_csv('/content/drive/MyDrive/STAT520/CondensedData_Filtered.csv')
vars_of_interest = ["FATHEDUC", "RACEGROUP", "FINCON_RC", "FirstGen_RC", "INCOME", "SIFMAJA"]
X = df[vars_of_interest].copy()

# Convert non-numerics to category codes
for col in X.columns:
    if not np.issubdtype(X[col].dtype, np.number):
        cat = X[col].astype("category")
        codes = cat.cat.codes.replace(-1, np.nan)
        X[col] = codes

res = little_mcar_test(X)
print(res)
if res["p_value"] < 0.05:
    print("Reject MCAR (data likely MAR or MNAR)")
else:
    print("Fail to reject MCAR (data consistent with MCAR)")

import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer  # noqa: F401
from sklearn.impute import IterativeImputer

# --------- 0) Load ORIGINAL (pre-imputation) dataset to get true missingness masks ---------
orig_path = "/content/drive/MyDrive/STAT520/CondensedData_Imputed.csv"
df0 = pd.read_csv(orig_path, low_memory=False)

# These are the ONLY variables we will impute/replace
targets = ["FATHEDUC", "RACEGROUP", "FINCON_RC", "FirstGen_RC", "INCOME", "SIFMAJA"]

# Save per-column masks of where values were missing originally
miss_masks = {col: df0[col].isna() for col in targets}

# --------- 1) Build the modeling matrix with ALL predictors (MAR) ---------
# Work on an encoded copy; leave df0 untouched so we can restore observed values later.
df_enc = df0.copy()

# Convert everything to numeric for the imputer (categoricals -> codes, keep NaN)
for col in df_enc.columns:
    if not np.issubdtype(df_enc[col].dtype, np.number):
        codes = df_enc[col].astype("category").cat.codes.replace(-1, np.nan)
        df_enc[col] = codes

# --------- 2) Fit MICE on the full matrix ---------
imputer = IterativeImputer(
    random_state=42,
    sample_posterior=True,
    max_iter=20
)
Z = pd.DataFrame(imputer.fit_transform(df_enc), columns=df_enc.columns, index=df_enc.index)

# --------- 3) Define valid code ranges (internal codes are 0..K-1) and map back to 1..K ---------
# If you know FATHEDUC code span, set it (example 1..6). If unknown, we infer from observed data below.
valid_ranges_1based = {
    "RACEGROUP":  (1, 7),
    "FINCON_RC":  (0, 2),   # Note: this is 0..2 by your codebook
    "FirstGen_RC":(0, 1),   # 0..1
    "INCOME":     (1, 14),
    "SIFMAJA":    (1, 16),
    "FATHEDUC": (1, 8),   # uncomment and adjust if you know the exact span
}

# helper to infer a plausible 1-based range if not specified
def infer_1based_range(s):
    s_num = pd.to_numeric(s, errors="coerce")
    lo = int(np.nanmin(s_num)) if np.isfinite(np.nanmin(s_num)) else 1
    hi = int(np.nanmax(s_num)) if np.isfinite(np.nanmax(s_num)) else lo
    return (lo, hi)

# Start from the original df0 and create a working output df
df = df0.copy()

for col in targets:
    # 3a) pull the imputed numeric column (floats) for this target
    imputed_f = pd.to_numeric(Z[col], errors="coerce")

    # 3b) determine intended 1-based (or 0-based) range
    if col in valid_ranges_1based:
        lo1, hi1 = valid_ranges_1based[col]
    else:
        lo1, hi1 = infer_1based_range(df0[col])

    # 3c) convert to internal 0..K-1 code space for rounding/clip, then map back
    # Cases:
    #   - If the codebook is 1..K: internal "codes" should be 0..K-1, so subtract 1 after rounding
    #   - If the codebook is 0..K: internal codes are already 0..K, no shift
    if lo1 == 1:
        # target codes are 1..K in your saved dataset; convert to 0..K-1 for rounding/clip
        K = hi1
        codes0 = np.rint(imputed_f - 1).astype("Int64")  # shift to 0-based BEFORE clip
        codes0 = codes0.clip(lower=0, upper=K-1)
        # now map back to 1..K for storage
        final_codes = (codes0 + 1).astype("Int64")
    elif lo1 == 0:
        # target codes are 0..K; just round and clip in 0-based space
        K = hi1
        codes0 = np.rint(imputed_f).astype("Int64").clip(lower=0, upper=K)
        final_codes = codes0.astype("Int64")
    else:
        # odd ranges: general fallback â€” round then clip to lo1..hi1 directly
        final_codes = np.rint(imputed_f).astype("Int64").clip(lower=lo1, upper=hi1)

    # 3d) CRITICAL: only fill rows that were missing originally
    mask = miss_masks[col]          # True where original df0 had NaN
    # Keep observed values exactly as they were in df0
    df.loc[~mask, col] = df0.loc[~mask, col]
    # Fill only the originally-missing rows with imputed codes
    df.loc[mask, col] = final_codes.loc[mask]

    # Optional: store as categorical so it prints cleanly and stays discrete
    df[col] = df[col].astype("Int64")
    df[col] = df[col].astype("category")

# --------- 4) Save the fixed dataset ---------
out_path = "/content/drive/MyDrive/STAT520/OfficialDataset_Final.csv"
df.to_csv(out_path, index=False)
print(f"Saved: {out_path}")

# Calculate the number of missing values (NaN) for each column in the final DataFrame
final_missing_values = df.isnull().sum()

# Convert the Series to a DataFrame for easier display
final_missing_values_table = final_missing_values.to_frame(name='NaN Count')

# Calculate the percentage of missing values for each column
total_rows_final = len(df)
final_missing_values_table['Percentage NaN'] = (final_missing_values_table['NaN Count'] / total_rows_final) * 100

# Format the 'Percentage NaN' column as a percentage string
final_missing_values_table['Percentage NaN'] = final_missing_values_table['Percentage NaN'].map('{:.2f}%'.format)

# Display the table
print("Table of NaN counts and percentages per column in the final dataset:")
display(final_missing_values_table)